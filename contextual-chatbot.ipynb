{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.stem.lancaster import LancasterStemmer\nstemmer = LancasterStemmer()\n\n# things we need for Tensorflow\nimport numpy as np\nimport tflearn\nimport tensorflow as tf\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"160a5901c0be4c2092e0ddb9530aee40ef743319"},"cell_type":"code","source":"# import our chat-bot intents file\nimport json\nwith open('../input/intents.json') as json_data:\n    intents = json.load(json_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10c02e35a8aa7d0ebbb044951159fb637ed0f684"},"cell_type":"code","source":"words = []\nclasses = []\ndocuments = []\nignore_words = ['?']\n# loop through each sentence in our intents patterns\nfor intent in intents['intents']:\n    for pattern in intent['patterns']:\n        # tokenize each word in the sentence\n        w = nltk.word_tokenize(pattern)\n        # add to our words list\n        words.extend(w)\n        # add to documents in our corpus\n        documents.append((w, intent['tag']))\n        # add to our classes list\n        if intent['tag'] not in classes:\n            classes.append(intent['tag'])\n\n# stem and lower each word and remove duplicates\nwords = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\nwords = sorted(list(set(words)))\n\n# remove duplicates\nclasses = sorted(list(set(classes)))\n\nprint (len(documents), \"documents\")\nprint (len(classes), \"classes\", classes)\nprint (len(words), \"unique stemmed words\", words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4e8fe1ac66c692bebbddae349c940fac63b9cf1"},"cell_type":"code","source":"# create our training data\ntraining = []\noutput = []\n# create an empty array for our output\noutput_empty = [0] * len(classes)\n\n# training set, bag of words for each sentence\nfor doc in documents:\n    # initialize our bag of words\n    bag = []\n    # list of tokenized words for the pattern\n    pattern_words = doc[0]\n    # stem each word\n    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n    # create our bag of words array\n    for w in words:\n        bag.append(1) if w in pattern_words else bag.append(0)\n\n    # output is a '0' for each tag and '1' for current tag\n    output_row = list(output_empty)\n    output_row[classes.index(doc[1])] = 1\n\n    training.append([bag, output_row])\n\n# shuffle our features and turn into np.array\nrandom.shuffle(training)\ntraining = np.array(training)\n\n# create train and test lists\ntrain_x = list(training[:,0])\ntrain_y = list(training[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e16f2af8f37caf1e400566a0dff4b7ad89ffbb31"},"cell_type":"code","source":"# reset underlying graph data\ntf.reset_default_graph()\n# Build neural network\nnet = tflearn.input_data(shape=[None, len(train_x[0])])\nnet = tflearn.fully_connected(net, 8)\nnet = tflearn.fully_connected(net, 8)\nnet = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\nnet = tflearn.regression(net)\n\n# Define model and setup tensorboard\nmodel = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n# Start training (apply gradient descent algorithm)\nmodel.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\nmodel.save('model.tflearn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ead0d2ca637ffa07147bad94fe67ae9cbbf822a4"},"cell_type":"code","source":"import pickle\npickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afa513a14f176f6a3d025902e2b96df377d8de30"},"cell_type":"code","source":"# restore all of our data structures\nimport pickle\ndata = pickle.load( open( \"training_data\", \"rb\" ) )\nwords = data['words']\nclasses = data['classes']\ntrain_x = data['train_x']\ntrain_y = data['train_y']\n\n# import our chat-bot intents file\nimport json\nwith open('../input/intents.json') as json_data:\n    intents = json.load(json_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9ace99858f0349c92efb6474a1e506908befde1"},"cell_type":"code","source":"# load our saved model\nmodel.load('./model.tflearn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"985c1c0319844955821aa8fd3c56b6b67ae4b031"},"cell_type":"code","source":"def clean_up_sentence(sentence):\n    # tokenize the pattern\n    sentence_words = nltk.word_tokenize(sentence)\n    # stem each word\n    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n    return sentence_words\n\n# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\ndef bow(sentence, words, show_details=False):\n    # tokenize the pattern\n    sentence_words = clean_up_sentence(sentence)\n    # bag of words\n    bag = [0]*len(words)  \n    for s in sentence_words:\n        for i,w in enumerate(words):\n            if w == s: \n                bag[i] = 1\n                if show_details:\n                    print (\"found in bag: %s\" % w)\n\n    return(np.array(bag))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed661764f46240293a772d08affec78f9162ffda"},"cell_type":"code","source":"p = bow(\"is your shop open today?\", words)\nprint (p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87a99bc431271c719a088dcbbe3055700bb431cc"},"cell_type":"code","source":"ERROR_THRESHOLD = 0.25\ndef classify(sentence):\n    # generate probabilities from the model\n    results = model.predict([bow(sentence, words)])[0]\n    # filter out predictions below a threshold\n    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n    # sort by strength of probability\n    results.sort(key=lambda x: x[1], reverse=True)\n    return_list = []\n    for r in results:\n        return_list.append((classes[r[0]], r[1]))\n    # return tuple of intent and probability\n    return return_list\n\ndef response(sentence, userID='123', show_details=False):\n    results = classify(sentence)\n    # if we have a classification then find the matching intent tag\n    if results:\n        # loop as long as there are matches to process\n        while results:\n            for i in intents['intents']:\n                # find a tag matching the first result\n                if i['tag'] == results[0][0]:\n                    # a random response from the intent\n                    return print(random.choice(i['responses']))\n\n            results.pop(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f997ee2f8d1a14f52fbf838bf757150e18a10423"},"cell_type":"code","source":"classify('is your shop open today?')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"765091a58a30564f0cf76b5fb87a5254f4e1eabc"},"cell_type":"code","source":"response('is your shop open today?')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7990cc24fadc1414f594ddfcc76a1f5e5aeba7b1"},"cell_type":"code","source":"response('Goodbye, see you later')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0e08f1453a46f5438ae6d87b846bdb47c8d8126"},"cell_type":"code","source":"# create a data structure to hold user context\ncontext = {}\n\nERROR_THRESHOLD = 0.25\ndef classify(sentence):\n    # generate probabilities from the model\n    results = model.predict([bow(sentence, words)])[0]\n    # filter out predictions below a threshold\n    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n    # sort by strength of probability\n    results.sort(key=lambda x: x[1], reverse=True)\n    return_list = []\n    for r in results:\n        return_list.append((classes[r[0]], r[1]))\n    # return tuple of intent and probability\n    return return_list\n\ndef response(sentence, userID='123', show_details=False):\n    results = classify(sentence)\n    # if we have a classification then find the matching intent tag\n    if results:\n        # loop as long as there are matches to process\n        while results:\n            for i in intents['intents']:\n                # find a tag matching the first result\n                if i['tag'] == results[0][0]:\n                    # set context for this intent if necessary\n                    if 'context_set' in i:\n                        if show_details: print ('context:', i['context_set'])\n                        context[userID] = i['context_set']\n\n                    # check if this intent is contextual and applies to this user's conversation\n                    if not 'context_filter' in i or \\\n                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n                        if show_details: print ('tag:', i['tag'])\n                        # a random response from the intent\n                        return print(random.choice(i['responses']))\n\n            results.pop(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"678c82be36001a491764555cd18ea49f956d6f69"},"cell_type":"code","source":"response('we want to rent a moped')\nprint ()\nresponse(\"Hi there!\", show_details=True)\nprint()\nclassify(\"Hi there!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aad8fe3395f2f2b1c53bc0185790da610c5e55c5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}